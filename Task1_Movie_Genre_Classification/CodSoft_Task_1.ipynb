{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX8HsOJEEDz-"
      },
      "outputs": [],
      "source": [
        "#Importing the essential Libraries and tools\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining File Path And File Structure\n",
        "from numpy.random import test\n",
        "DATA_DIR = \"/content/Data\"\n",
        "\n",
        "train_path = os.path.join(DATA_DIR, \"train_data.txt\")\n",
        "test_path = os.path.join(DATA_DIR, \"test_data.txt\")\n",
        "solution_path = os.path.join(DATA_DIR, \"test_data_solution.txt\")\n",
        "\n",
        "print(\"Train File Exists: \", os.path.exists(train_path))\n",
        "print(\"Test File Exists: \", os.path.exists(test_path))\n",
        "print(\"Solution:\", os.path.exists(solution_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoZ0ZgY8Jy5A",
        "outputId": "a48580e1-6528-4fdb-c9ec-b9e60bab6486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train File Exists:  True\n",
            "Test File Exists:  True\n",
            "Solution: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the dataset\n",
        "train_df = pd.read_csv(\n",
        "    train_path,\n",
        "    sep=\":::\",\n",
        "    engine=\"python\",\n",
        "    header=None,\n",
        "    names=[\"title\", \"genre\", \"plot\"]\n",
        ")\n",
        "\n",
        "test_df = pd.read_csv(\n",
        "    test_path,\n",
        "    sep=\":::\",\n",
        "    engine=\"python\",\n",
        "    header=None,\n",
        "    names=[\"title\", \"genre\", \"plot\"]\n",
        ")\n",
        "\n",
        "solution_df = pd.read_csv(\n",
        "    solution_path,\n",
        "    sep=\":::\",\n",
        "    engine=\"python\",\n",
        "    header=None,\n",
        "    names=[\"title\", \"genre\", \"plot\"]\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test  shape:\", test_df.shape)\n",
        "print(\"Solution shape:\", solution_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBNkbYK_K9zi",
        "outputId": "743e5d21-c2cf-4e81-c32b-ee99587a9c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (54214, 3)\n",
            "Test  shape: (54200, 3)\n",
            "Solution shape: (54200, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick look at the data\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eddcFPJnNiBI",
        "outputId": "16f5d70f-a44d-4ee6-9c8a-cded20a8c5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                title  ...                                               plot\n",
            "1       Oscar et la dame rose (2009)   ...   Listening in to a conversation between his do...\n",
            "2                       Cupid (1997)   ...   A brother and sister with a past incestuous r...\n",
            "3   Young, Wild and Wonderful (1980)   ...   As the bus empties the students for their fie...\n",
            "4              The Secret Sin (1915)   ...   To help their unemployed father make ends mee...\n",
            "5             The Unrecovered (2007)   ...   The film's title refers not only to the un-re...\n",
            "\n",
            "[5 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nGenre value counts (train):\")\n",
        "print(train_df[\"genre\"].value_counts().head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYUMYHCwNzFu",
        "outputId": "cb37f42d-d1e0-44e5-c6f6-b925823d9970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Genre value counts (train):\n",
            "genre\n",
            "drama           13613\n",
            "documentary     13096\n",
            "comedy           7447\n",
            "short            5073\n",
            "horror           2204\n",
            "thriller         1591\n",
            "action           1315\n",
            "western          1032\n",
            "reality-tv        884\n",
            "family            784\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/Validation Split of the dataset\n",
        "X = train_df[\"plot\"]\n",
        "y = train_df[\"genre\"]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\nTrain samples:\", len(X_train))\n",
        "print(\"Val samples:\", len(X_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyaWvc-lOTu2",
        "outputId": "01ab8771-38c7-43f8-8406-dc66d4eaec9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train samples: 43371\n",
            "Val samples: 10843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Models\n",
        "model = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"Linear SVC\": LinearSVC()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "best_model = None\n",
        "best_name = None\n",
        "best_acc = 0"
      ],
      "metadata": {
        "id": "Wry4sxH7RAuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Each Model on Training Datasplit\n",
        "from sklearn import pipeline\n",
        "for name, clf in model.items():\n",
        "  print(\"Training:\", name)\n",
        "\n",
        "\n",
        "  pipeline = Pipeline([\n",
        "      (\"tfidf\", TfidfVectorizer(\n",
        "          stop_words=\"english\",\n",
        "          max_features=20000,\n",
        "          ngram_range=(1, 2)\n",
        "      )),\n",
        "      (\"clf\", clf)\n",
        "\n",
        "      ])\n",
        "\n",
        "  pipeline.fit(X_train, y_train)\n",
        "  preds = pipeline.predict(X_val)\n",
        "\n",
        "  acc = accuracy_score(y_val, preds)\n",
        "  print(f\"Validation Accuracy = {acc:.4f}\")\n",
        "  print(classification_report(y_val, preds))\n",
        "\n",
        "  results[name] = acc\n",
        "\n",
        "  if acc > best_acc:\n",
        "    best_acc = acc\n",
        "    best_model = pipeline\n",
        "    best_name = name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdOzdErmR0iN",
        "outputId": "c12d9746-8229-448a-9d66-9ec06d105225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: Logistic Regression\n",
            "Validation Accuracy = 0.5817\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      action        0.55      0.24      0.33       263\n",
            "       adult        0.74      0.26      0.39       118\n",
            "   adventure        0.67      0.12      0.20       155\n",
            "   animation        0.57      0.04      0.07       100\n",
            "   biography        0.00      0.00      0.00        53\n",
            "      comedy        0.52      0.58      0.55      1490\n",
            "       crime        0.25      0.01      0.02       101\n",
            " documentary        0.66      0.87      0.75      2619\n",
            "       drama        0.54      0.79      0.64      2723\n",
            "      family        0.57      0.08      0.13       157\n",
            "     fantasy        0.00      0.00      0.00        65\n",
            "   game-show        1.00      0.33      0.50        39\n",
            "     history        0.00      0.00      0.00        49\n",
            "      horror        0.69      0.56      0.62       441\n",
            "       music        0.70      0.35      0.47       146\n",
            "     musical        1.00      0.02      0.04        55\n",
            "     mystery        0.00      0.00      0.00        64\n",
            "        news        1.00      0.06      0.11        36\n",
            "  reality-tv        0.46      0.15      0.22       177\n",
            "     romance        0.00      0.00      0.00       134\n",
            "      sci-fi        0.62      0.22      0.32       129\n",
            "       short        0.49      0.31      0.38      1015\n",
            "       sport        0.61      0.20      0.30        86\n",
            "   talk-show        0.80      0.05      0.10        78\n",
            "    thriller        0.39      0.12      0.19       318\n",
            "         war        0.00      0.00      0.00        26\n",
            "     western        0.95      0.62      0.75       206\n",
            "\n",
            "     accuracy                           0.58     10843\n",
            "    macro avg       0.51      0.22      0.26     10843\n",
            " weighted avg       0.56      0.58      0.53     10843\n",
            "\n",
            "Training: Naive Bayes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy = 0.5017\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      action        1.00      0.02      0.03       263\n",
            "       adult        0.50      0.01      0.02       118\n",
            "   adventure        0.67      0.03      0.05       155\n",
            "   animation        0.00      0.00      0.00       100\n",
            "   biography        0.00      0.00      0.00        53\n",
            "      comedy        0.54      0.38      0.44      1490\n",
            "       crime        0.00      0.00      0.00       101\n",
            " documentary        0.55      0.91      0.69      2619\n",
            "       drama        0.44      0.84      0.58      2723\n",
            "      family        0.00      0.00      0.00       157\n",
            "     fantasy        0.00      0.00      0.00        65\n",
            "   game-show        0.00      0.00      0.00        39\n",
            "     history        0.00      0.00      0.00        49\n",
            "      horror        0.85      0.15      0.25       441\n",
            "       music        1.00      0.01      0.03       146\n",
            "     musical        0.00      0.00      0.00        55\n",
            "     mystery        0.00      0.00      0.00        64\n",
            "        news        0.00      0.00      0.00        36\n",
            "  reality-tv        0.00      0.00      0.00       177\n",
            "     romance        0.00      0.00      0.00       134\n",
            "      sci-fi        0.00      0.00      0.00       129\n",
            "       short        0.77      0.07      0.12      1015\n",
            "       sport        0.55      0.07      0.12        86\n",
            "   talk-show        0.00      0.00      0.00        78\n",
            "    thriller        0.00      0.00      0.00       318\n",
            "         war        0.00      0.00      0.00        26\n",
            "     western        1.00      0.23      0.37       206\n",
            "\n",
            "     accuracy                           0.50     10843\n",
            "    macro avg       0.29      0.10      0.10     10843\n",
            " weighted avg       0.50      0.50      0.40     10843\n",
            "\n",
            "Training: Linear SVC\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy = 0.5682\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      action        0.43      0.37      0.40       263\n",
            "       adult        0.64      0.43      0.52       118\n",
            "   adventure        0.40      0.22      0.28       155\n",
            "   animation        0.33      0.15      0.21       100\n",
            "   biography        0.00      0.00      0.00        53\n",
            "      comedy        0.52      0.57      0.54      1490\n",
            "       crime        0.24      0.07      0.11       101\n",
            " documentary        0.68      0.81      0.74      2619\n",
            "       drama        0.56      0.69      0.62      2723\n",
            "      family        0.31      0.13      0.19       157\n",
            "     fantasy        0.14      0.05      0.07        65\n",
            "   game-show        0.82      0.59      0.69        39\n",
            "     history        0.29      0.04      0.07        49\n",
            "      horror        0.60      0.62      0.61       441\n",
            "       music        0.57      0.47      0.52       146\n",
            "     musical        0.38      0.05      0.10        55\n",
            "     mystery        0.13      0.03      0.05        64\n",
            "        news        0.38      0.08      0.14        36\n",
            "  reality-tv        0.40      0.26      0.32       177\n",
            "     romance        0.21      0.05      0.08       134\n",
            "      sci-fi        0.47      0.36      0.41       129\n",
            "       short        0.42      0.34      0.38      1015\n",
            "       sport        0.61      0.47      0.53        86\n",
            "   talk-show        0.35      0.17      0.23        78\n",
            "    thriller        0.28      0.17      0.21       318\n",
            "         war        0.40      0.08      0.13        26\n",
            "     western        0.85      0.80      0.82       206\n",
            "\n",
            "     accuracy                           0.57     10843\n",
            "    macro avg       0.42      0.30      0.33     10843\n",
            " weighted avg       0.54      0.57      0.54     10843\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing Which Model is Best\n",
        "for k, v in results.items():\n",
        "    print(f\"{k:20s} : {v:.4f}\")\n",
        "print(f\"Best Model: {best_name} with accuracy = {best_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4OdToAWUIy2",
        "outputId": "9e056f50-f3fc-4315-a96c-74263faaf25c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression  : 0.5817\n",
            "Naive Bayes          : 0.5017\n",
            "Linear SVC           : 0.5682\n",
            "Best Model: Logistic Regression with accuracy = 0.5817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Evaluation on Test Set\n",
        "X_test = test_df[\"plot\"]\n",
        "y_test = solution_df[\"genre\"]\n",
        "\n",
        "test_preds = best_model.predict(X_test)     # Using best model = Logistic regresstion\n",
        "\n",
        "test_acc = accuracy_score(y_test, test_preds)\n",
        "print(f\"Test Accuracy = {test_acc:.4f}\")\n",
        "print(classification_report(y_test, test_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32NB3Uu8ZAaz",
        "outputId": "373ec7c8-8773-4191-e61c-2600254ac677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy = 0.5843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "      action        0.54      0.25      0.34      1314\n",
            "       adult        0.64      0.19      0.30       590\n",
            "   adventure        0.72      0.14      0.24       775\n",
            "   animation        0.53      0.03      0.06       498\n",
            "   biography        0.00      0.00      0.00       264\n",
            "      comedy        0.53      0.58      0.56      7446\n",
            "       crime        0.50      0.02      0.03       505\n",
            " documentary        0.66      0.87      0.75     13096\n",
            "       drama        0.53      0.80      0.64     13612\n",
            "      family        0.57      0.06      0.11       783\n",
            "     fantasy        0.75      0.01      0.02       322\n",
            "   game-show        0.93      0.45      0.61       193\n",
            "     history        0.00      0.00      0.00       243\n",
            "      horror        0.67      0.55      0.60      2204\n",
            "       music        0.72      0.39      0.50       731\n",
            "     musical        0.50      0.00      0.01       276\n",
            "     mystery        1.00      0.00      0.01       318\n",
            "        news        0.89      0.04      0.08       181\n",
            "  reality-tv        0.51      0.12      0.20       883\n",
            "     romance        0.56      0.01      0.03       672\n",
            "      sci-fi        0.62      0.21      0.31       646\n",
            "       short        0.51      0.32      0.39      5072\n",
            "       sport        0.81      0.20      0.32       431\n",
            "   talk-show        0.66      0.10      0.17       391\n",
            "    thriller        0.40      0.10      0.17      1590\n",
            "         war        0.00      0.00      0.00       132\n",
            "     western        0.93      0.68      0.79      1032\n",
            "\n",
            "     accuracy                           0.58     54200\n",
            "    macro avg       0.58      0.23      0.27     54200\n",
            " weighted avg       0.58      0.58      0.54     54200\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}